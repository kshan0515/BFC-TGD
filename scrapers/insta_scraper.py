"""
BFC-TGD (Bucheon FC 1995 Integrated Search Agent)
Copyright (c) 2026 kshan0515. Licensed under the MIT License.
Created with â¤ï¸ for Bucheon FC 1995 Fans.
"""
import os
import datetime
import base64
from apify_client import ApifyClient
from instaloader import Instaloader, Hashtag, Post
from pymongo import MongoClient, UpdateOne

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
MONGO_URI = os.getenv('MONGO_URI')
APIFY_TOKEN = os.getenv('APIFY_TOKEN')
INSTA_USER = os.getenv('INSTA_USER') 
INSTA_SESSION_64 = os.getenv('INSTA_SESSION_64') 
DB_NAME = 'bfc-tgd'

# ì¸ìŠ¤íƒ€ê·¸ë¨ ì°¨ë‹¨ ë°©ì§€ë¥¼ ìœ„í•œ ê³ ì • User-Agent
USER_AGENT = "Mozilla/5.0 (iPhone; CPU iPhone OS 16_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Mobile/15E148 Safari/604.1"

def load_session_from_env(L, username):
    """GitHub Secretsì—ì„œ Base64 ì„¸ì…˜ì„ ì½ì–´ íŒŒì¼ë¡œ ë³µêµ¬ ë° ë¡œë“œ"""
    if not INSTA_SESSION_64:
        print("âš ï¸ Skip Session Load: INSTA_SESSION_64 is not set.")
        return False

    try:
        session_path = f"/tmp/session-{username}"
        # ì¤„ë°”ê¿ˆ ì œê±° í›„ ë””ì½”ë”© (ì˜¤ì—¼ ë°©ì§€)
        clean_session = INSTA_SESSION_64.strip().replace("\n", "").replace("\r", "")
        with open(session_path, "wb") as f:
            f.write(base64.b64decode(clean_session))
        
        L.load_session_from_file(username, filename=session_path)
        print(f"âœ… [Session] Successfully restored session for {username}")
        return True
    except Exception as e:
        print(f"âŒ [Session] Failed to load session: {e}")
        return False

def scrape_via_apify(tags):
    """Apifyë¥¼ ì‚¬ìš©í•˜ì—¬ ì•ˆì „í•˜ê²Œ ì¸ìŠ¤íƒ€ê·¸ë¨ ìˆ˜ì§‘"""
    if not APIFY_TOKEN:
        print("âš ï¸ Skip Apify: APIFY_TOKEN is not set.")
        return []

    print(f"ğŸš€ [Apify] Starting ultra-optimized scrape for tags: {tags}")
    apify_client = ApifyClient(APIFY_TOKEN)
    
    run_input = {
        "hashtags": tags,
        "resultsLimit": 30,
    }
    
    try:
        run = apify_client.actor("apify/instagram-hashtag-scraper").call(run_input=run_input)
        time_threshold = datetime.datetime.utcnow() - datetime.timedelta(hours=2)
        
        collected_data = []
        for item in apify_client.dataset(run["defaultDatasetId"]).iterate_items():
            ts_str = item.get("timestamp")
            if not ts_str: continue
            
            pub_date = datetime.datetime.fromisoformat(ts_str.replace("Z", "+00:00"))
            if pub_date.replace(tzinfo=None) < time_threshold:
                continue

            collected_data.append({
                "external_id": item.get("shortCode"),
                "platform": "INSTA",
                "type": "IMAGE" if item.get("type") != "Video" else "VIDEO",
                "title": None,
                "caption": item.get("caption"),
                "media_uri": item.get("displayUrl"),
                "origin_url": item.get("url"),
                "published_at": pub_date, 
                "username": item.get("ownerUsername"),
                "metadata": {
                    "shortcode": item.get("shortCode"),
                    "likes": item.get("likesCount"),
                    "comments": item.get("commentsCount")
                }
            })
        return collected_data
    except Exception as e:
        print(f"ğŸ“¡ Apify API Error: {e}")
        return []

def scrape_via_instaloader(tag_name):
    """Instaloaderë¥¼ ì‚¬ìš©í•œ ì§ì ‘ ìˆ˜ì§‘ (ê°œì„ ëœ ë£¨í”„ ì ìš©)"""
    print(f"ğŸš€ [Instaloader] Starting idiomatic scrape for #{tag_name}")
    # User-Agent ì£¼ì…ìœ¼ë¡œ ë´‡ íƒì§€ ì™„í™”
    L = Instaloader(user_agent=USER_AGENT)
    
    if INSTA_USER:
        load_session_from_env(L, INSTA_USER)

    since = datetime.datetime.utcnow() - datetime.timedelta(hours=2)
    
    try:
        hashtag = Hashtag.from_name(L.context, tag_name)
        posts = hashtag.get_posts()
        
        collected_data = []
        # takewhile ëŒ€ì‹  ìƒìœ„ 50ê°œë¥¼ í›‘ìœ¼ë©° ì‹œê°„ í•„í„°ë§ (ë¹„ì—°ì†ì„± ëŒ€ì‘)
        count = 0
        for post in posts:
            count += 1
            if count > 50: break # ìµœëŒ€ 50ê°œê¹Œì§€ë§Œ í™•ì¸
            
            # 2ì‹œê°„ ì´ë‚´ ê²Œì‹œë¬¼ë§Œ ì¶”ê°€
            if post.date_utc > since:
                collected_data.append({
                    "external_id": post.shortcode,
                    "platform": "INSTA",
                    "type": "IMAGE" if not post.is_video else "VIDEO",
                    "title": None,
                    "caption": post.caption,
                    "media_uri": post.url,
                    "origin_url": f"https://www.instagram.com/p/{post.shortcode}/",
                    "published_at": post.date_utc,
                    "username": post.owner_username,
                    "metadata": {
                        "shortcode": post.shortcode,
                        "likes": post.likes,
                        "comments": post.comments,
                        "is_video": post.is_video
                    }
                })
                print(f"ğŸ“¦ Found: {post.shortcode} ({post.date_utc})")
            
        return collected_data
    except Exception as e:
        print(f"âŒ Instaloader Error for #{tag_name}: {e}")
        return []

def save_to_mongo(data):
    if not data:
        print("âš ï¸ No data to save.")
        return

    client = MongoClient(MONGO_URI)
    db = client[DB_NAME]
    collection = db['contents']

    operations = []
    for item in data:
        if not item['external_id']: continue
        
        item['updated_at'] = datetime.datetime.utcnow()
        operations.append(
            UpdateOne(
                {"external_id": item['external_id']},
                {"$set": item},
                upsert=True
            )
        )

    if operations:
        result = collection.bulk_write(operations)
        print(f"âœ… Successfully synced {len(data)} items to MongoDB.")
        print(f"ğŸ“Š Stats - Upserted: {result.upserted_count}, Matched: {result.matched_count}")

def main():
    tags = ['ë¶€ì²œFC', 'ë¶€ì²œFC1995']
    data = []
    
    # 1. ìš°ì„  ì•ˆì •ì ì¸ Apifyë¡œ ì‹œë„
    data = scrape_via_apify(tags)

    # 2. Apify ì‹¤íŒ¨ ì‹œì—ë§Œ ë‚´ ê³„ì •(Instaloader Session)ìœ¼ë¡œ ë°±ì—… ì‹¤í–‰
    if not data:
        print("ğŸ”„ [Backup] Apify is unavailable. Switching to Instaloader session mode...")
        for t in tags:
            data.extend(scrape_via_instaloader(t))
            
    # 3. ì €ì¥
    if data:
        save_to_mongo(data)
    else:
        print("âš ï¸ No data collected from any source.")

if __name__ == "__main__":
    main()
