"""
BFC-TGD (Bucheon FC 1995 Integrated Search Agent)
Copyright (c) 2026 kshan0515. Licensed under the MIT License.
Created with â¤ï¸ for Bucheon FC 1995 Fans.
"""
import os
import datetime
from apify_client import ApifyClient
from instaloader import Instaloader, Hashtag
from pymongo import MongoClient, UpdateOne

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
MONGO_URI = os.getenv('MONGO_URI')
APIFY_TOKEN = os.getenv('APIFY_TOKEN')
INSTA_USER = os.getenv('INSTA_USER') # ì˜µì…˜: ë¡œê·¸ì¸ìš© ì•„ì´ë””
INSTA_PASS = os.getenv('INSTA_PASS') # ì˜µì…˜: ë¡œê·¸ì¸ìš© ë¹„ë°€ë²ˆí˜¸
DB_NAME = 'bfc-tgd'

def scrape_via_apify(tags):
    """Apifyë¥¼ ì‚¬ìš©í•˜ì—¬ ì•ˆì „í•˜ê²Œ ì¸ìŠ¤íƒ€ê·¸ë¨ ìˆ˜ì§‘ (ë¹„ìš© ê·¹ëŒ€í™” ìµœì í™”)"""
    if not APIFY_TOKEN:
        print("âš ï¸ Skip Apify: APIFY_TOKEN is not set.")
        return []

    # --- ë¹„ìš© ìµœì í™” Pre-check (íƒ€ì´íŠ¸í•œ 110ë¶„ ì ìš©) ---
    client = MongoClient(MONGO_URI)
    db = client[DB_NAME]
    last_item = db['contents'].find_one(
        {"platform": "INSTA"},
        sort=[("updated_at", -1)]
    )
    
    if last_item and "updated_at" in last_item:
        time_diff = datetime.datetime.utcnow() - last_item["updated_at"]
        if time_diff < datetime.timedelta(minutes=110):
            print(f"â˜• Scraped recently ({time_diff.seconds // 60}m ago). Skipping to save Apify credits.")
            return []
    # -----------------------------------------------

    print(f"ğŸš€ [Apify] Starting ultra-optimized scrape for tags: {tags}")
    apify_client = ApifyClient(APIFY_TOKEN)
    
    run_input = {
        "hashtags": tags,
        "resultsLimit": 10, # 2ì‹œê°„ ì£¼ê¸° ë‚´ì˜ ì‹ ê·œë¬¼ë§Œ íƒ€ê²ŸíŒ… (ìµœì†Œ ë¹„ìš©)
    }
    
    run = apify_client.actor("apify/instagram-hashtag-scraper").call(run_input=run_input)
    
    # ìµœê·¼ 2ì‹œê°„ ì´ë‚´ì˜ ê²Œì‹œë¬¼ë§Œ ìˆ˜ì§‘í•˜ë„ë¡ ì‹œê°„ ê¸°ì¤€ ì„¤ì •
    time_threshold = datetime.datetime.utcnow() - datetime.timedelta(hours=2)
    
    collected_data = []
    for item in apify_client.dataset(run["defaultDatasetId"]).iterate_items():
        # ë‚ ì§œ ì²´í¬: Apifyê°€ ê°€ì ¸ì˜¨ ë°ì´í„° ì¤‘ì—ì„œë„ ë„ˆë¬´ ì˜¤ë˜ëœ ê²ƒì€ ì œì™¸
        # íƒ€ì„ìŠ¤íƒ¬í”„ í˜•ì‹ ì²˜ë¦¬ (Z -> +00:00)
        ts_str = item.get("timestamp")
        if not ts_str: continue
        
        pub_date = datetime.datetime.fromisoformat(ts_str.replace("Z", "+00:00"))
        if pub_date.replace(tzinfo=None) < time_threshold:
            continue

        collected_data.append({
            "external_id": item.get("shortCode"),
            "platform": "INSTA",
            "type": "IMAGE" if item.get("type") != "Video" else "VIDEO",
            "title": None,
            "caption": item.get("caption"),
            "media_uri": item.get("displayUrl"),
            "origin_url": item.get("url"),
            "published_at": pub_date, # ë¬¸ìì—´ ëŒ€ì‹  datetime ê°ì²´ ì €ì¥
            "username": item.get("ownerUsername"),
            "metadata": {
                "shortcode": item.get("shortCode"),
                "likes": item.get("likesCount"),
                "comments": item.get("commentsCount")
            }
        })
    return collected_data

def scrape_via_instaloader(tag_name):
    """Instaloaderë¥¼ ì‚¬ìš©í•œ ì§ì ‘ ìˆ˜ì§‘ (ë¡œê·¸ì¸ ì˜µì…˜ í¬í•¨)"""
    print(f"ğŸš€ [Instaloader] Starting scrape for #{tag_name}")
    L = Instaloader()
    
    if INSTA_USER and INSTA_PASS:
        try:
            L.login(INSTA_USER, INSTA_PASS)
            print(f"âœ… Logged in as {INSTA_USER}")
        except Exception as e:
            print(f"âš ï¸ Login failed: {e}. Attempting as anonymous...")

    two_hours_ago = datetime.datetime.utcnow() - datetime.timedelta(hours=2)
    hashtag = Hashtag.from_name(L.context, tag_name)
    
    collected_data = []
    for post in hashtag.get_posts():
        if post.date_utc < two_hours_ago:
            break
        
        collected_data.append({
            "external_id": post.shortcode,
            "platform": "INSTA",
            "type": "IMAGE" if not post.is_video else "VIDEO",
            "title": None,
            "caption": post.caption,
            "media_uri": post.url,
            "origin_url": f"https://www.instagram.com/p/{post.shortcode}/",
            "published_at": post.date_utc,
            "username": post.owner_username,
            "metadata": {
                "shortcode": post.shortcode,
                "likes": post.likes,
                "comments": post.comments
            }
        })
    return collected_data

def save_to_mongo(data):
    if not data:
        print("âš ï¸ No data to save.")
        return

    client = MongoClient(MONGO_URI)
    db = client[DB_NAME]
    collection = db['contents']

    operations = []
    for item in data:
        if not item['external_id']: continue
        
        item['updated_at'] = datetime.datetime.utcnow()
        operations.append(
            UpdateOne(
                {"external_id": item['external_id']},
                {"$set": item},
                upsert=True
            )
        )

    if operations:
        result = collection.bulk_write(operations)
        print(f"âœ… Successfully synced {len(data)} items to MongoDB.")
        print(f"ğŸ“Š Stats - Upserted: {result.upserted_count}, Matched: {result.matched_count}")

if __name__ == "__main__":
    tags = ['ë¶€ì²œFC', 'ë¶€ì²œFC1995']
    
    # 1. ìš°ì„  Apifyë¡œ ì‹œë„
    data = scrape_via_apify(tags)
    
    # 2. Apify í† í°ì´ ì—†ê±°ë‚˜ ê²°ê³¼ê°€ ì—†ì„ ê²½ìš° (ì˜µì…˜) ì§ì ‘ ìˆ˜ì§‘ ì‹œë„
    if not data and INSTA_USER:
        print("ğŸ”„ Falling back to direct Instaloader scrape...")
        for t in tags:
            data.extend(scrape_via_instaloader(t))
            
    # 3. ì €ì¥
    save_to_mongo(data)
