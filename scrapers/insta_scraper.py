"""
BFC-TGD (Bucheon FC 1995 Integrated Search Agent)
Copyright (c) 2026 kshan0515. Licensed under the MIT License.
Created with â¤ï¸ for Bucheon FC 1995 Fans.
"""
import os
import datetime
import base64
from itertools import takewhile
from apify_client import ApifyClient
from instaloader import Instaloader, Hashtag, Post
from pymongo import MongoClient, UpdateOne

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
MONGO_URI = os.getenv('MONGO_URI')
APIFY_TOKEN = os.getenv('APIFY_TOKEN')
INSTA_USER = os.getenv('INSTA_USER') # ì¸ìŠ¤íƒ€ê·¸ë¨ ì•„ì´ë”” (í•„ìˆ˜)
INSTA_SESSION_64 = os.getenv('INSTA_SESSION_64') # ìµœê³ ê¸‰ ì„¸ì…˜ í…ìŠ¤íŠ¸ (Base64)
DB_NAME = 'bfc-tgd'

def load_session_from_env(L, username):
    """GitHub Secretsì—ì„œ Base64 ì„¸ì…˜ì„ ì½ì–´ íŒŒì¼ë¡œ ë³µêµ¬ ë° ë¡œë“œ"""
    if not INSTA_SESSION_64:
        print("âš ï¸ Skip Session Load: INSTA_SESSION_64 is not set.")
        return False

    try:
        # 1. ì„ì‹œ ê²½ë¡œì— ì„¸ì…˜ íŒŒì¼ ë³µêµ¬
        session_path = f"/tmp/session-{username}"
        with open(session_path, "wb") as f:
            f.write(base64.b64decode(INSTA_SESSION_64))
        
        # 2. ì„¸ì…˜ ë¡œë“œ (ë¡œê·¸ì¸ ëŒ€ì²´)
        L.load_session_from_file(username, filename=session_path)
        print(f"âœ… [Session] Successfully restored session for {username}")
        return True
    except Exception as e:
        print(f"âŒ [Session] Failed to load session: {e}")
        return False

def scrape_via_apify(tags):
    """Apifyë¥¼ ì‚¬ìš©í•˜ì—¬ ì•ˆì „í•˜ê²Œ ì¸ìŠ¤íƒ€ê·¸ë¨ ìˆ˜ì§‘ (ë¹„ìš© ìµœì í™” ì ìš©)"""
    if not APIFY_TOKEN:
        print("âš ï¸ Skip Apify: APIFY_TOKEN is not set.")
        return []

    # --- ìµœê·¼ ì—…ë°ì´íŠ¸ ì—¬ë¶€ ì²´í¬ ìƒëµ (2ì‹œê°„ ì£¼ê¸°ë©´ ë¬´ì¡°ê±´ ëŒë¦¬ëŠ” ê²Œ ì•ˆì „) ---
    print(f"ğŸš€ [Apify] Starting ultra-optimized scrape for tags: {tags}")
    apify_client = ApifyClient(APIFY_TOKEN)
    
    run_input = {
        "hashtags": tags,
        "resultsLimit": 30, # 2ì‹œê°„ ì£¼ê¸° ë‚´ì˜ ì‹ ê·œë¬¼ ëˆ„ë½ ë°©ì§€ë¥¼ ìœ„í•´ 30ê°œë¡œ ìƒí–¥
    }
    
    try:
        run = apify_client.actor("apify/instagram-hashtag-scraper").call(run_input=run_input)
        
        # ìµœê·¼ 2ì‹œê°„ ì´ë‚´ì˜ ê²Œì‹œë¬¼ë§Œ ìˆ˜ì§‘í•˜ë„ë¡ ì‹œê°„ ê¸°ì¤€ ì„¤ì •
        time_threshold = datetime.datetime.utcnow() - datetime.timedelta(hours=2)
        
        collected_data = []
        for item in apify_client.dataset(run["defaultDatasetId"]).iterate_items():
            ts_str = item.get("timestamp")
            if not ts_str: continue
            
            pub_date = datetime.datetime.fromisoformat(ts_str.replace("Z", "+00:00"))
            if pub_date.replace(tzinfo=None) < time_threshold:
                continue

            collected_data.append({
                "external_id": item.get("shortCode"),
                "platform": "INSTA",
                "type": "IMAGE" if item.get("type") != "Video" else "VIDEO",
                "title": None,
                "caption": item.get("caption"),
                "media_uri": item.get("displayUrl"),
                "origin_url": item.get("url"),
                "published_at": pub_date, 
                "username": item.get("ownerUsername"),
                "metadata": {
                    "shortcode": item.get("shortCode"),
                    "likes": item.get("likesCount"),
                    "comments": item.get("commentsCount")
                }
            })
        return collected_data
    except Exception as e:
        print(f"ğŸ“¡ Apify API Error: {e}")
        return []

def scrape_via_instaloader(tag_name):
    """Instaloaderë¥¼ ì‚¬ìš©í•œ ì§ì ‘ ìˆ˜ì§‘ (ì„¸ì…˜ ë³µêµ¬ ë¡œì§ í¬í•¨)"""
    print(f"ğŸš€ [Instaloader] Starting idiomatic scrape for #{tag_name}")
    L = Instaloader()
    
    # 1. ì„¸ì…˜ ë¡œë”© ì‹œë„ (ì•„ì´ë””/ë¹„ë²ˆ ë¡œê·¸ì¸ë³´ë‹¤ í›¨ì”¬ ì•ˆì „)
    if INSTA_USER:
        load_session_from_env(L, INSTA_USER)

    # 2. ìˆ˜ì§‘ ê¸°ì¤€ ì‹œê°„ (2ì‹œê°„ ì „)
    since = datetime.datetime.utcnow() - datetime.timedelta(hours=2)
    
    try:
        hashtag = Hashtag.from_name(L.context, tag_name)
        posts = hashtag.get_posts()
        
        collected_data = []
        # takewhileì„ ì‚¬ìš©í•˜ì—¬ 2ì‹œê°„ ì´ì „ ê²Œì‹œë¬¼ ë°œê²¬ ì‹œ ì¦‰ì‹œ ì¢…ë£Œ
        for post in takewhile(lambda p: p.date_utc > since, posts):
            collected_data.append({
                "external_id": post.shortcode,
                "platform": "INSTA",
                "type": "IMAGE" if not post.is_video else "VIDEO",
                "title": None,
                "caption": post.caption,
                "media_uri": post.url,
                "origin_url": f"https://www.instagram.com/p/{post.shortcode}/",
                "published_at": post.date_utc,
                "username": post.owner_username,
                "metadata": {
                    "shortcode": post.shortcode,
                    "likes": post.likes,
                    "comments": post.comments,
                    "is_video": post.is_video
                }
            })
            print(f"ğŸ“¦ Found: {post.shortcode}")
            
        return collected_data
    except Exception as e:
        print(f"âŒ Instaloader Error: {e}")
        return []

def save_to_mongo(data):
    if not data:
        print("âš ï¸ No data to save.")
        return

    client = MongoClient(MONGO_URI)
    db = client[DB_NAME]
    collection = db['contents']

    operations = []
    for item in data:
        if not item['external_id']: continue
        
        item['updated_at'] = datetime.datetime.utcnow()
        operations.append(
            UpdateOne(
                {"external_id": item['external_id']},
                {"$set": item},
                upsert=True
            )
        )

    if operations:
        result = collection.bulk_write(operations)
        print(f"âœ… Successfully synced {len(data)} items to MongoDB.")
        print(f"ğŸ“Š Stats - Upserted: {result.upserted_count}, Matched: {result.matched_count}")

def main():
    tags = ['ë¶€ì²œFC', 'ë¶€ì²œFC1995']
    data = []
    
    # 1. ìš°ì„  ì•ˆì •ì ì¸ Apifyë¡œ ì‹œë„
    data = scrape_via_apify(tags)

    # 2. Apify ì‹¤íŒ¨ ì‹œì—ë§Œ ë‚´ ê³„ì •(Instaloader Session)ìœ¼ë¡œ ë°±ì—… ì‹¤í–‰
    if not data:
        print("ğŸ”„ [Backup] Apify is unavailable. Switching to Instaloader session mode...")
        for t in tags:
            data.extend(scrape_via_instaloader(t))
            
    # 3. ì €ì¥
    if data:
        save_to_mongo(data)
    else:
        print("âš ï¸ No data collected from any source.")

if __name__ == "__main__":
    main()
