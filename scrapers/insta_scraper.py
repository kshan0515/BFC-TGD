"""
BFC-TGD (Bucheon Football Village - ë¶€ì²œ ì¶•êµ¬ë™)
Copyright (c) 2026 kshan0515. Licensed under the MIT License.
Created with â¤ï¸ for Bucheon FC 1995 Fans.
"""
import os
import datetime
from apify_client import ApifyClient
from pymongo import MongoClient, UpdateOne
from dotenv import load_dotenv

# .env.local ë˜ëŠ” .env íŒŒì¼ ë¡œë“œ (ë¡œì»¬ ê°œë°œìš©)
env_paths = [".env.local", ".env", "../.env.local", "../.env"]
for path in env_paths:
    if os.path.exists(path):
        load_dotenv(path)
        break

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
MONGO_URI = os.getenv('MONGO_URI')
APIFY_TOKEN = os.getenv('APIFY_TOKEN')
DB_NAME = 'bfc-tgd'

def scrape_via_apify(tags):
    """Apifyë¥¼ ì‚¬ìš©í•˜ì—¬ ì•ˆì „í•˜ê²Œ ì¸ìŠ¤íƒ€ê·¸ë¨ ìˆ˜ì§‘ (ìµœì¢… ìµœì í™” ë²„ì „)"""
    if not APIFY_TOKEN:
        print("âŒ Error: APIFY_TOKEN is not set.")
        return None

    print(f"ğŸš€ [Apify] Starting scrape for tags: {tags}")
    apify_client = ApifyClient(APIFY_TOKEN)
    
    run_input = {
        "hashtags": tags,
        "resultsLimit": 20,
    }
    
    try:
        run = apify_client.actor("apify/instagram-hashtag-scraper").call(
            run_input=run_input,
            timeout_secs=180, # 3ë¶„ íƒ€ì„ì•„ì›ƒ ë°©ì–´ë§‰
            memory_mbytes=256 # ë¹„ìš© ì ˆê° ë©”ëª¨ë¦¬ ì„¤ì •
        )
        
        collected_data = []
        for item in apify_client.dataset(run["defaultDatasetId"]).iterate_items():
            # ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´ í•„ë“œ ì¶”ì¶œ (ë°©ì–´ ì½”ë“œ)
            short_code = item.get("shortCode") or item.get("shortcode")
            display_url = item.get("displayUrl") or item.get("display_url")
            timestamp = item.get("timestamp") or item.get("taken_at_timestamp")
            
            if not short_code or not timestamp: continue
            
            # íƒ€ì„ìŠ¤íƒ¬í”„ ì²˜ë¦¬
            try:
                if isinstance(timestamp, str):
                    pub_date = datetime.datetime.fromisoformat(timestamp.replace("Z", "+00:00"))
                else:
                    pub_date = datetime.datetime.fromtimestamp(timestamp)
            except:
                pub_date = datetime.datetime.utcnow()

            collected_data.append({
                "external_id": short_code,
                "platform": "INSTA",
                "type": "IMAGE" if item.get("type", "").lower() != "video" else "VIDEO",
                "title": None,
                "caption": item.get("caption", ""),
                "media_uri": display_url,
                "origin_url": item.get("url") or f"https://www.instagram.com/p/{short_code}/",
                "published_at": pub_date, 
                "username": item.get("ownerUsername") or item.get("owner_username", "instagram_user"),
                "metadata": {
                    "shortcode": short_code,
                    "likes": item.get("likesCount") or item.get("likes_count", 0),
                    "comments": item.get("commentsCount") or item.get("comments_count", 0)
                }
            })
        
        print(f"ğŸ“¦ [Apify] Parsed {len(collected_data)} items successfully.")
        return collected_data
    except Exception as e:
        print(f"ğŸ“¡ Apify API Error: {e}")
        return None

def save_to_mongo(data):
    if not data:
        print("âš ï¸ No data to save.")
        return

    client = MongoClient(MONGO_URI)
    db = client[DB_NAME]
    collection = db['contents']

    operations = []
    for item in data:
        if not item['external_id']: continue
        item['updated_at'] = datetime.datetime.utcnow()
        operations.append(UpdateOne({"external_id": item['external_id']}, {"$set": item}, upsert=True))

    if operations:
        result = collection.bulk_write(operations)
        print(f"âœ… Successfully synced {len(data)} items to MongoDB. (Upserted: {result.upserted_count})")

def main():
    tags = ['ë¶€ì²œFC']
    
    # ì˜¤ì§ Apifyë¡œë§Œ ì •ì •ë‹¹ë‹¹í•˜ê²Œ(?) ìˆ˜ì§‘ ì‹œë„
    data = scrape_via_apify(tags)

    if data:
        save_to_mongo(data)
    elif data == []:
        print("âœ… Apify run successful, but returned 0 items.")
    else:
        print("âš ï¸ Scraping failed. No data to save.")

if __name__ == "__main__":
    main()
