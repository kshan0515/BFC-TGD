import os
import datetime
from googleapiclient.discovery import build
from pymongo import MongoClient, UpdateOne

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (GitHub Secrets ë˜ëŠ” Local .env)
YOUTUBE_API_KEY = os.getenv('YOUTUBE_API_KEY')
MONGO_URI = os.getenv('MONGO_URI')
DB_NAME = 'bfc-tgd'

def scrape_youtube(keyword='ë¶€ì²œFC'):
    if not YOUTUBE_API_KEY or not MONGO_URI:
        print("âŒ Error: YOUTUBE_API_KEY or MONGO_URI environment variable is not set.")
        return

    # 1ì£¼ì¼ ì „ ì‹œê°„ ê³„ì‚° (RFC 3339 í˜•ì‹)
    time_threshold = (datetime.datetime.utcnow() - datetime.timedelta(days=7)).isoformat() + "Z"
    print(f"ğŸš€ Starting YouTube scrape for keyword: {keyword}")
    print(f"ğŸ“… Fetching videos published after: {time_threshold} (Last 7 days)")

    try:
        # 1. ìœ íŠœë¸Œ API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        youtube = build('youtube', 'v3', developerKey=YOUTUBE_API_KEY)
        
        # 2. ì˜ìƒ ê²€ìƒ‰ (ìµœëŒ€ 200ê°œ ìˆ˜ì§‘ì„ ìœ„í•œ í˜ì´ì§€ë„¤ì´ì…˜)
        collected_items = []
        next_page_token = None
        max_total_results = 200
        
        while len(collected_items) < max_total_results:
            request = youtube.search().list(
                part="snippet",
                q=keyword,
                maxResults=min(50, max_total_results - len(collected_items)), # API ì œí•œì¸ 50ê°œì”© ìš”ì²­
                type="video",
                order="date",
                publishedAfter=time_threshold,
                pageToken=next_page_token
            )
            response = request.execute()
            
            items = response.get('items', [])
            if not items:
                break
                
            collected_items.extend(items)
            next_page_token = response.get('nextPageToken')
            
            print(f"ğŸ“¦ Collected {len(collected_items)} / {max_total_results} items...")
            
            if not next_page_token:
                break

        # 3. MongoDB Atlas ì—°ê²°
        client = MongoClient(MONGO_URI)
        db = client[DB_NAME]
        collection = db['contents']

        operations = []
        for item in collected_items:
            video_id = item['id']['videoId']
            snippet = item['snippet']
            
            # ë°ì´í„° êµ¬ì¡° ìƒì„±
            content_doc = {
                "external_id": video_id,
                "platform": "YOUTUBE",
                "type": "VIDEO",
                "title": snippet['title'],
                "caption": snippet['description'],
                "media_uri": snippet['thumbnails']['high']['url'],
                "origin_url": f"https://www.youtube.com/watch?v={video_id}",
                "published_at": snippet['publishedAt'],
                "username": snippet['channelTitle'],
                "metadata": {
                    "channel_id": snippet['channelId'],
                    "videoId": video_id
                },
                "updated_at": datetime.datetime.utcnow()
            }

            operations.append(
                UpdateOne(
                    {"external_id": video_id},
                    {"$set": content_doc},
                    upsert=True
                )
            )

        # 4. ë²Œí¬ ì‹¤í–‰
        if operations:
            result = collection.bulk_write(operations)
            print(f"âœ… [v2.0] Final Success! Total {len(collected_items)} videos processed.")
            print(f"ğŸ“Š Stats - Upserted: {result.upserted_count}, Matched: {result.matched_count}")
        else:
            print("âš ï¸ No videos found in the last week.")

    except Exception as e:
        print(f"âŒ Critical Error: {str(e)}")

if __name__ == "__main__":
    scrape_youtube()
