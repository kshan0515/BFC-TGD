"""
BFC-TGD (Bucheon Football Village - ë¶€ì²œ ì¶•êµ¬ë™)
Copyright (c) 2026 kshan0515. Licensed under the MIT License.
Created with â¤ï¸ for Bucheon FC 1995 Fans.
"""
import os
import datetime
from googleapiclient.discovery import build
from pymongo import MongoClient, UpdateOne

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (GitHub Secrets ë˜ëŠ” Local .env)
YOUTUBE_API_KEY = os.getenv('YOUTUBE_API_KEY')
MONGO_URI = os.getenv('MONGO_URI')
DB_NAME = 'bfc-tgd'

def scrape_youtube():
    # keywords = ['ë¶€ì²œFC', 'ë¶€ì²œFC1995', 'BFC1995']
    keywords = ['ë¶€ì²œFC', 'ë¶€ì²œFC1995']
    if not YOUTUBE_API_KEY or not MONGO_URI:
        print("âŒ Error: YOUTUBE_API_KEY or MONGO_URI environment variable is not set.")
        return

    # 48ì‹œê°„ ì „ ì‹œê°„ ê³„ì‚° (ìœ íŠœë¸Œ ê²€ìƒ‰ API ì¸ë±ì‹± ì§€ì—° ëŒ€ë¹„)
    time_threshold = (datetime.datetime.utcnow() - datetime.timedelta(hours=48)).isoformat() + "Z"
    print(f"ğŸš€ Starting YouTube scrape for keywords: {', '.join(keywords)}")
    print(f"ğŸ“… Fetching videos published after: {time_threshold} (Last 48 hours)")

    try:
        # 1. ìœ íŠœë¸Œ API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        youtube = build('youtube', 'v3', developerKey=YOUTUBE_API_KEY)
        
        all_collected_items = []
        
        for keyword in keywords:
            print(f"ğŸ” Searching for: {keyword}...")
            collected_for_keyword = []
            next_page_token = None
            max_per_keyword = 50 # ê° í‚¤ì›Œë“œë³„ ìµœëŒ€ 50ê°œ
            
            while len(collected_for_keyword) < max_per_keyword:
                request = youtube.search().list(
                    part="snippet",
                    q=keyword,
                    maxResults=50,
                    type="video",
                    order="date",
                    publishedAfter=time_threshold,
                    pageToken=next_page_token
                )
                response = request.execute()
                
                items = response.get('items', [])
                if not items:
                    break
                    
                collected_for_keyword.extend(items)
                next_page_token = response.get('nextPageToken')
                
                if not next_page_token:
                    break
            
            all_collected_items.extend(collected_for_keyword)
            print(f"âœ… Found {len(collected_for_keyword)} items for '{keyword}'")

        # 2. MongoDB Atlas ì—°ê²°
        client = MongoClient(MONGO_URI)
        db = client[DB_NAME]
        collection = db['contents']

        operations = []
        for item in all_collected_items:
            video_id = item['id']['videoId']
            snippet = item['snippet']
            channel_title = snippet.get('channelTitle', '')
            
            # ë¸”ë™ë¦¬ìŠ¤íŠ¸ ì±„ë„ í•„í„°ë§ (ì €ì¥ ì•ˆí•¨)
            if is_excluded_channel(channel_title):
                print(f"ğŸš« ë¸”ë™ë¦¬ìŠ¤íŠ¸ ì±„ë„ ì˜ìƒ ê±´ë„ˆëœ€: {channel_title} - {snippet.get('title')}")
                continue
            
            # ë‚ ì§œ íŒŒì‹±: ISO ë¬¸ìì—´ì„ datetime ê°ì²´ë¡œ ë³€í™˜
            pub_date_str = snippet['publishedAt']
            pub_date = datetime.datetime.fromisoformat(pub_date_str.replace("Z", "+00:00"))

            content_doc = {
                "external_id": video_id,
                "platform": "YOUTUBE",
                "type": "VIDEO",
                "title": snippet['title'],
                "caption": snippet['description'],
                "media_uri": snippet['thumbnails']['high']['url'],
                "origin_url": f"https://www.youtube.com/watch?v={video_id}",
                "published_at": pub_date, # datetime ê°ì²´ë¡œ ì €ì¥
                "username": snippet['channelTitle'],
                "metadata": {
                    "channel_id": snippet['channelId'],
                    "videoId": video_id
                },
                "updated_at": datetime.datetime.utcnow()
            }

            operations.append(
                UpdateOne(
                    {"external_id": video_id},
                    {"$set": content_doc},
                    upsert=True
                )
            )

        # 3. ë²Œí¬ ì‹¤í–‰
        if operations:
            result = collection.bulk_write(operations)
            print(f"âœ… [v2.3] Final Success! Total {len(all_collected_items)} records processed.")
            print(f"ğŸ“Š Stats - Upserted: {result.upserted_count}, Matched: {result.matched_count}")
        else:
            print("âš ï¸ No videos found for any keywords in the last week.")

    except Exception as e:
        print(f"âŒ Critical Error: {str(e)}")

def is_excluded_channel(channel_title):
    # ì œì™¸í•  ì±„ë„ëª… ë¦¬ìŠ¤íŠ¸ (ë¶€ë¶„ ì¼ì¹˜ ë˜ëŠ” ì •í™•í•œ ì¼ì¹˜ ê°€ëŠ¥)
    EXCLUDED_CHANNELS = ['ì•ˆì§€í™˜2015', 'ë¶€ì²œìœ ë‚˜ì´í‹°ë“œ']
    
    for excluded in EXCLUDED_CHANNELS:
        if excluded in channel_title:
            return True
    return False

if __name__ == "__main__":
    scrape_youtube()
